"""
í•™êµ ê³µì§€ì‚¬í•­ í¬ë¡¤ë§ ë° AI íŒŒì‹± ì„œë¹„ìŠ¤
- ëŒ€í•™êµ í™ˆí˜ì´ì§€ í¬ë¡¤ë§
- AI ê¸°ë°˜ ì¤‘ìš” ê³µì§€ í•„í„°ë§
- ì¼ì •ì— ì˜í–¥ ì£¼ëŠ” ë‚´ìš© ìë™ ì¶”ì¶œ
"""

import os
import json
import re
import asyncio
from datetime import datetime, timedelta
from typing import List, Dict, Optional
from dataclasses import dataclass
from urllib.parse import urljoin

import httpx
from bs4 import BeautifulSoup
import google.generativeai as genai
from dotenv import load_dotenv

load_dotenv()

GOOGLE_API_KEY = os.getenv("GOOGLE_API_KEY")
GEMINI_MODEL_NAME = os.getenv("GEMINI_MODEL_NAME", "gemini-1.5-flash")

genai.configure(api_key=GOOGLE_API_KEY)


@dataclass
class Notice:
    """ê³µì§€ì‚¬í•­"""
    title: str
    url: str
    date: datetime
    content: Optional[str]
    source: str  # í•™ì‚¬ê³µì§€, ì¥í•™ê³µì§€ ë“±
    
    def to_dict(self):
        return {
            "title": self.title,
            "url": self.url,
            "date": self.date.strftime("%Y-%m-%d"),
            "content": self.content,
            "source": self.source
        }


@dataclass
class ImportantNotice:
    """ì¤‘ìš” ê³µì§€ (AI ë¶„ì„ ê²°ê³¼)"""
    notice: Notice
    importance_level: str  # critical, high, medium, low
    category: str  # íœ´ê°•, ì„±ì , ìˆ˜ê°•ì‹ ì²­, ì¥í•™ê¸ˆ, í–‰ì‚¬, ê¸°íƒ€
    summary: str
    action_required: bool
    deadline: Optional[datetime]
    affects_schedule: bool
    suggested_action: Optional[str]
    
    def to_dict(self):
        return {
            "notice": self.notice.to_dict(),
            "importance_level": self.importance_level,
            "category": self.category,
            "summary": self.summary,
            "action_required": self.action_required,
            "deadline": self.deadline.strftime("%Y-%m-%d") if self.deadline else None,
            "affects_schedule": self.affects_schedule,
            "suggested_action": self.suggested_action
        }


class UniversityNoticeConfig:
    """ëŒ€í•™ë³„ í¬ë¡¤ë§ ì„¤ì •"""
    
    # ìƒ˜í”Œ ëŒ€í•™ ì„¤ì • (ì‹¤ì œ ì‚¬ìš© ì‹œ í™•ì¥)
    UNIVERSITIES = {
        "konkuk": {
            "name": "ê±´êµ­ëŒ€í•™êµ",
            "base_url": "https://www.konkuk.ac.kr",
            "notice_urls": {
                "í•™ì‚¬ê³µì§€": "/do/MessageBoard/ArticleList.do?forum=notice",
                "ì¥í•™ê³µì§€": "/do/MessageBoard/ArticleList.do?forum=scholarship"
            },
            "selectors": {
                "list": "table.board-list tbody tr",
                "title": "td.subject a",
                "date": "td.date",
                "link_prefix": "/do/MessageBoard/ArticleRead.do"
            }
        },
        "yonsei": {
            "name": "ì—°ì„¸ëŒ€í•™êµ",
            "base_url": "https://www.yonsei.ac.kr",
            "notice_urls": {
                "í•™ì‚¬ê³µì§€": "/sc/support/notice.jsp"
            },
            "selectors": {
                "list": "ul.board-list li",
                "title": "a.title",
                "date": "span.date"
            }
        },
        "snu": {
            "name": "ì„œìš¸ëŒ€í•™êµ",
            "base_url": "https://www.snu.ac.kr",
            "notice_urls": {
                "í•™ì‚¬ê³µì§€": "/snunow/notice/gennotice"
            },
            "selectors": {
                "list": "ul.list-container li",
                "title": "a",
                "date": "span.date"
            }
        },
        # ë” ë§ì€ ëŒ€í•™ ì¶”ê°€ ê°€ëŠ¥
        "custom": {
            "name": "ì‚¬ìš©ì ì§€ì •",
            "base_url": "",
            "notice_urls": {},
            "selectors": {}
        }
    }
    
    @classmethod
    def get_config(cls, university_code: str) -> dict:
        return cls.UNIVERSITIES.get(university_code, cls.UNIVERSITIES["custom"])


class NoticeCrawler:
    """ê³µì§€ì‚¬í•­ í¬ë¡¤ëŸ¬"""
    
    def __init__(self, university_code: str = "konkuk"):
        self.config = UniversityNoticeConfig.get_config(university_code)
        self.client = httpx.AsyncClient(
            timeout=30.0,
            headers={
                "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
            }
        )
    
    async def crawl_notices(self, notice_type: str = "í•™ì‚¬ê³µì§€", limit: int = 20) -> List[Notice]:
        """ê³µì§€ì‚¬í•­ í¬ë¡¤ë§"""
        notices = []
        
        url_path = self.config["notice_urls"].get(notice_type)
        if not url_path:
            return notices
        
        full_url = urljoin(self.config["base_url"], url_path)
        
        try:
            response = await self.client.get(full_url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            selectors = self.config["selectors"]
            
            items = soup.select(selectors["list"])[:limit]
            
            for item in items:
                try:
                    # ì œëª©ê³¼ ë§í¬
                    title_elem = item.select_one(selectors["title"])
                    if not title_elem:
                        continue
                    
                    title = title_elem.get_text(strip=True)
                    link = title_elem.get("href", "")
                    if link and not link.startswith("http"):
                        link = urljoin(self.config["base_url"], link)
                    
                    # ë‚ ì§œ
                    date_elem = item.select_one(selectors["date"])
                    date_str = date_elem.get_text(strip=True) if date_elem else ""
                    date = self._parse_date(date_str)
                    
                    notices.append(Notice(
                        title=title,
                        url=link,
                        date=date,
                        content=None,
                        source=notice_type
                    ))
                except Exception as e:
                    print(f"Notice parse error: {e}")
                    continue
            
        except Exception as e:
            print(f"Crawl error: {e}")
        
        return notices
    
    async def get_notice_content(self, notice: Notice) -> str:
        """ê³µì§€ì‚¬í•­ ë³¸ë¬¸ ì¡°íšŒ"""
        try:
            response = await self.client.get(notice.url)
            response.raise_for_status()
            
            soup = BeautifulSoup(response.text, "html.parser")
            
            # ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸° (ì¼ë°˜ì ì¸ ì„ íƒìë“¤)
            content_selectors = [
                "div.board-content",
                "div.view-content",
                "div.content-view",
                "div.article-body",
                "td.content"
            ]
            
            for selector in content_selectors:
                content_elem = soup.select_one(selector)
                if content_elem:
                    return content_elem.get_text(strip=True)[:2000]  # ìµœëŒ€ 2000ì
            
            return ""
        except Exception as e:
            print(f"Content fetch error: {e}")
            return ""
    
    def _parse_date(self, date_str: str) -> datetime:
        """ë‚ ì§œ íŒŒì‹±"""
        # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì²˜ë¦¬
        patterns = [
            r"(\d{4})[.-](\d{1,2})[.-](\d{1,2})",  # 2026-01-31 or 2026.01.31
            r"(\d{1,2})[.-](\d{1,2})",  # 01-31 (ì˜¬í•´ë¡œ ê°€ì •)
        ]
        
        for pattern in patterns:
            match = re.search(pattern, date_str)
            if match:
                groups = match.groups()
                if len(groups) == 3:
                    return datetime(int(groups[0]), int(groups[1]), int(groups[2]))
                elif len(groups) == 2:
                    return datetime(datetime.now().year, int(groups[0]), int(groups[1]))
        
        return datetime.now()
    
    async def close(self):
        await self.client.aclose()


class NoticeAnalyzer:
    """ê³µì§€ì‚¬í•­ AI ë¶„ì„ê¸°"""
    
    def __init__(self):
        self.model = genai.GenerativeModel(
            model_name=GEMINI_MODEL_NAME,
            generation_config={
                "temperature": 0.3,
                "response_mime_type": "application/json"
            }
        )
    
    async def analyze_notices(self, notices: List[Notice]) -> List[ImportantNotice]:
        """ê³µì§€ì‚¬í•­ ì¼ê´„ ë¶„ì„"""
        if not notices:
            return []
        
        # ê³µì§€ ë°ì´í„° ì¤€ë¹„
        notice_data = [
            {
                "index": i,
                "title": n.title,
                "date": n.date.strftime("%Y-%m-%d"),
                "content": n.content[:500] if n.content else "",
                "source": n.source
            }
            for i, n in enumerate(notices)
        ]
        
        prompt = f"""
        ëŒ€í•™ ê³µì§€ì‚¬í•­ì„ ë¶„ì„í•˜ì—¬ í•™ìƒ ì¼ì •ì— ì˜í–¥ì„ ì£¼ëŠ” ì¤‘ìš”í•œ ê³µì§€ë¥¼ ì‹ë³„í•´ì£¼ì„¸ìš”.
        
        í˜„ì¬ ë‚ ì§œ: {datetime.now().strftime("%Y-%m-%d")}
        
        [ê³µì§€ì‚¬í•­ ëª©ë¡]
        {json.dumps(notice_data, ensure_ascii=False)}
        
        [ë¶„ì„ ê¸°ì¤€]
        - íœ´ê°•, ë³´ê°•: ìˆ˜ì—… ì¼ì • ë³€ê²½
        - ì„±ì  í™•ì¸/ì´ì˜ì‹ ì²­ ê¸°ê°„: ì¤‘ìš” ë§ˆê°
        - ìˆ˜ê°•ì‹ ì²­/ë³€ê²½ ê¸°ê°„: ë§¤ìš° ì¤‘ìš”
        - ì¥í•™ê¸ˆ ì‹ ì²­: ë§ˆê°ì¼ ì¤‘ìš”
        - ì‹œí—˜ ê´€ë ¨: ì¼ì • ì˜í–¥
        - í–‰ì‚¬/íŠ¹ê°•: ì„ íƒì 
        
        [ì¤‘ìš”ë„ ë ˆë²¨]
        - critical: ìˆ˜ê°•ì‹ ì²­, ì„±ì  ì´ì˜ì‹ ì²­ ë“± ê¸°í•œ ì¤‘ìš”
        - high: íœ´ê°•, ë³´ê°•, ì‹œí—˜ ë³€ê²½
        - medium: ì¥í•™ê¸ˆ, í–‰ì‚¬
        - low: ì¼ë°˜ ì•ˆë‚´
        
        [OUTPUT JSON FORMAT]
        {{
            "important_notices": [
                {{
                    "index": 0,
                    "importance_level": "critical|high|medium|low",
                    "category": "íœ´ê°•|ë³´ê°•|ì„±ì |ìˆ˜ê°•ì‹ ì²­|ì¥í•™ê¸ˆ|ì‹œí—˜|í–‰ì‚¬|ê¸°íƒ€",
                    "summary": "í•œ ì¤„ ìš”ì•½",
                    "action_required": true|false,
                    "deadline": "YYYY-MM-DD ë˜ëŠ” null",
                    "affects_schedule": true|false,
                    "suggested_action": "ì¶”ì²œ í–‰ë™ ë˜ëŠ” null"
                }}
            ]
        }}
        
        ì¤‘ìš”ë„ medium ì´ìƒë§Œ í¬í•¨í•´ì£¼ì„¸ìš”.
        """
        
        try:
            response = self.model.generate_content(prompt)
            result = json.loads(response.text)
            
            important_notices = []
            for item in result.get("important_notices", []):
                idx = item.get("index", 0)
                if idx < len(notices):
                    deadline = None
                    if item.get("deadline"):
                        try:
                            deadline = datetime.strptime(item["deadline"], "%Y-%m-%d")
                        except:
                            pass
                    
                    important_notices.append(ImportantNotice(
                        notice=notices[idx],
                        importance_level=item.get("importance_level", "medium"),
                        category=item.get("category", "ê¸°íƒ€"),
                        summary=item.get("summary", notices[idx].title),
                        action_required=item.get("action_required", False),
                        deadline=deadline,
                        affects_schedule=item.get("affects_schedule", False),
                        suggested_action=item.get("suggested_action")
                    ))
            
            return important_notices
            
        except Exception as e:
            print(f"Notice analysis error: {e}")
            return []
    
    async def summarize_for_user(self, important_notices: List[ImportantNotice]) -> str:
        """ì‚¬ìš©ììš© ìš”ì•½ ë©”ì‹œì§€ ìƒì„±"""
        if not important_notices:
            return "ğŸ“¢ ì´ë²ˆ ì£¼ ì¤‘ìš” ê³µì§€ì‚¬í•­ì´ ì—†ìŠµë‹ˆë‹¤."
        
        # ì¤‘ìš”ë„ë³„ ë¶„ë¥˜
        critical = [n for n in important_notices if n.importance_level == "critical"]
        high = [n for n in important_notices if n.importance_level == "high"]
        
        summary_parts = ["ğŸ“¢ **ì¤‘ìš” ê³µì§€ì‚¬í•­ ì•Œë¦¼**\n"]
        
        if critical:
            summary_parts.append("ğŸš¨ **ê¸´ê¸‰**")
            for n in critical:
                deadline_str = f" (ë§ˆê°: {n.deadline.strftime('%m/%d')})" if n.deadline else ""
                summary_parts.append(f"- {n.summary}{deadline_str}")
            summary_parts.append("")
        
        if high:
            summary_parts.append("âš ï¸ **ì¤‘ìš”**")
            for n in high:
                summary_parts.append(f"- {n.summary}")
        
        return "\n".join(summary_parts)


class NoticeService:
    """ê³µì§€ì‚¬í•­ í†µí•© ì„œë¹„ìŠ¤"""
    
    def __init__(self, university_code: str = "konkuk"):
        self.crawler = NoticeCrawler(university_code)
        self.analyzer = NoticeAnalyzer()
    
    async def get_important_notices(
        self, 
        notice_types: List[str] = None,
        fetch_content: bool = False
    ) -> List[ImportantNotice]:
        """ì¤‘ìš” ê³µì§€ì‚¬í•­ ì¡°íšŒ"""
        if notice_types is None:
            notice_types = ["í•™ì‚¬ê³µì§€"]
        
        all_notices = []
        
        for notice_type in notice_types:
            notices = await self.crawler.crawl_notices(notice_type)
            
            # ìµœê·¼ 2ì£¼ ì´ë‚´ ê³µì§€ë§Œ
            cutoff = datetime.now() - timedelta(days=14)
            notices = [n for n in notices if n.date >= cutoff]
            
            # ë³¸ë¬¸ ì¡°íšŒ (ì„ íƒì )
            if fetch_content:
                for notice in notices[:10]:  # ìµœëŒ€ 10ê°œë§Œ
                    notice.content = await self.crawler.get_notice_content(notice)
            
            all_notices.extend(notices)
        
        # AI ë¶„ì„
        important = await self.analyzer.analyze_notices(all_notices)
        
        # ì¤‘ìš”ë„ ìˆœ ì •ë ¬
        importance_order = {"critical": 0, "high": 1, "medium": 2, "low": 3}
        important.sort(key=lambda x: importance_order.get(x.importance_level, 3))
        
        return important
    
    async def get_daily_digest(self) -> Dict:
        """ì¼ì¼ ê³µì§€ ë‹¤ì´ì œìŠ¤íŠ¸"""
        important = await self.get_important_notices(
            notice_types=["í•™ì‚¬ê³µì§€", "ì¥í•™ê³µì§€"],
            fetch_content=True
        )
        
        summary = await self.analyzer.summarize_for_user(important)
        
        return {
            "date": datetime.now().strftime("%Y-%m-%d"),
            "summary": summary,
            "notices": [n.to_dict() for n in important],
            "total_count": len(important),
            "action_required_count": sum(1 for n in important if n.action_required)
        }
    
    async def close(self):
        await self.crawler.close()


# í¸ì˜ í•¨ìˆ˜
async def fetch_university_notices(university_code: str = "konkuk") -> Dict:
    """ëŒ€í•™ ê³µì§€ì‚¬í•­ ì¡°íšŒ (í¸ì˜ í•¨ìˆ˜)"""
    service = NoticeService(university_code)
    try:
        return await service.get_daily_digest()
    finally:
        await service.close()
