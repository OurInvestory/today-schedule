import json
import os
import re
import warnings
from datetime import datetime, timedelta
from collections import defaultdict
from fastapi import APIRouter, UploadFile, File, HTTPException
from fastapi.concurrency import run_in_threadpool
from dotenv import load_dotenv

# IBM Watsonx SDK
from ibm_watsonx_ai.foundation_models import ModelInference
from ibm_watsonx_ai.metanames import GenTextParamsMetaNames as GenParams

from app.schemas.ai_chat import APIResponse, ChatResponseData, AIChatParsed

warnings.filterwarnings("ignore")
load_dotenv()

router = APIRouter()

# --- [ì´ˆê¸°í™”] OCR Reader (Optional) ---
try:
    import easyocr
    ocr_reader = easyocr.Reader(['ko', 'en'], gpu=False)
    OCR_AVAILABLE = True
except ImportError:
    ocr_reader = None
    OCR_AVAILABLE = False
    print("Warning: easyocr not installed. Image analysis will be limited.")

# ëª¨ë¸ ID
WATSONX_MODEL_ID_TEXT = os.getenv("WATSONX_MODEL_ID")  # 70b-instruct (OCR í…ìŠ¤íŠ¸ ë¶„ì„ìš©)

credentials = {
    "url": os.getenv("WATSONX_URL"),
    "apikey": os.getenv("WATSONX_API_KEY")
}
project_id = os.getenv("WATSONX_PROJECT_ID")

def detect_image_type(ocr_results) -> str:
    """ì´ë¯¸ì§€ íƒ€ì… ê°ì§€: 'schedule' ë˜ëŠ” 'poster'"""
    texts = [text for (_, text, _) in ocr_results]
    text_combined = " ".join(texts).lower()
    text_no_space = text_combined.replace(" ", "")
    
    # 1. [ê°•ë ¥í•œ ì‹ í˜¸] í¬ìŠ¤í„° í‚¤ì›Œë“œê°€ ëª…í™•í•˜ë©´ ìš°ì„ ë°˜í™˜
    # ê³µë°± ì œê±° ë²„ì „ì—ì„œë„ ì²´í¬ (OCRì´ "ê³µ ëª¨ ì „"ìœ¼ë¡œ ì½ì„ ìˆ˜ ìˆìŒ)
    strong_poster_keywords = ['ê³µëª¨ì „', 'ëŒ€ì™¸í™œë™', 'ì„œí¬í„°ì¦ˆ', 'ì±„ìš©', 'ì½˜í…ŒìŠ¤íŠ¸', 'competition', 'recruit',
                              'ìƒê¸ˆ', 'ìƒì¥', 'ì‹œìƒ', 'ì£¼ìµœ', 'í›„ì›', 'ì‘ëª¨']
    if any(kw in text_combined or kw in text_no_space for kw in strong_poster_keywords):
        return 'poster'

    # 2. Schedule ì ìˆ˜ ê³„ì‚°
    schedule_keywords = ['ì‹œê°„í‘œ', 'ê°•ì˜ì‹¤', 'êµì‹œ', 'timetable', 'class', 'lecture', 'professor', 'í•™ê¸°', 'semester']
    schedule_score = sum(1 for kw in schedule_keywords if kw in text_combined)
    
    # [ê°œì„ ] ìš”ì¼ ê°ì§€ - ë‚ ì§œ í˜•ì‹(ì›”), (í™”)ëŠ” ì œì™¸í•˜ê³ , ë…ë¦½ì ì¸ ìš”ì¼ë§Œ ì¹´ìš´íŠ¸
    # ë‚ ì§œ í˜•ì‹ì˜ ê´„í˜¸ ìš”ì¼ ì œê±° í›„ ì²´í¬
    import re
    text_cleaned = re.sub(r'\([ì›”í™”ìˆ˜ëª©ê¸ˆí† ì¼]\)', '', text_combined)
    
    days_kor = ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ']
    days_eng = ['mon', 'tue', 'wed', 'thu', 'fri']
    
    # ë…ë¦½ì ì¸ ìš”ì¼ ë°œê²¬ (ê³µë°±ì´ë‚˜ ë‹¨ì–´ ê²½ê³„ë¡œ êµ¬ë¶„)
    found_days = 0
    for d in days_kor:
        # ë…ë¦½ì ìœ¼ë¡œ ë“±ì¥í•˜ëŠ” ìš”ì¼ë§Œ (ì˜ˆ: "ì›” í™” ìˆ˜" ë˜ëŠ” ë‹¨ë… ì…€)
        if re.search(rf'(^|\s){d}($|\s)', text_cleaned):
            found_days += 1
    for d in days_eng:
        if re.search(rf'\b{d}\b', text_cleaned):
            found_days += 1
        
    if found_days >= 3:
        schedule_score += 5  # ê°•ë ¥í•œ ê°€ì‚°ì  ë¶€ì—¬

    # 3. Poster ì ìˆ˜ ê³„ì‚°
    poster_keywords = ['ëª¨ì§‘', 'ì‹ ì²­', 'í–‰ì‚¬', 'ë§ˆê°', 'ë¬¸ì˜', 'ì§€ì›', 'ê¸°ê°„', 'í™œë™', 'ë´‰ì‚¬', 'ê³µëª¨', 'ìƒë‹´', 'ì ‘ìˆ˜', 'ë°œí‘œ']
    poster_score = sum(1 for kw in poster_keywords if kw in text_combined)
    
    # 4. ì ìˆ˜ ë¹„êµ (ë™ì ì´ë©´ Schedule ìš°ì„ ìœ¼ë¡œ ë³€ê²½ - ì¼ë°˜ì ì¸ í…ìŠ¤íŠ¸ ì—†ëŠ” ë¬¸ì„œ ë“±ì€ ì‹œê°„í‘œ ë¶„ì„ íë¦„ì´ ë” ê´€ëŒ€í•¨)
    return 'schedule' if schedule_score >= poster_score else 'poster'

def is_header_or_noise(text):
    text = text.replace(" ", "").strip()
    if text in ['ì›”', 'í™”', 'ìˆ˜', 'ëª©', 'ê¸ˆ', 'í† ', 'ì¼', 'Mon', 'Tue', 'Wed', 'Thu', 'Fri']: return True
    if text.isdigit(): return True
    if re.match(r'^[\W_]+$', text): return True
    if text in ['shl', 'ê¸°', 'ë¹„ê³ ', 'ì‹œê°„', 'ë¯¸ê¸°ì¬', 'ìƒë‹´', 'êµì‹œ', 'ê³¼ëª©ëª…']: return True 

    if re.match(r'^[A-Z]\d{3,4}$', text): return True
    
    if re.search(r'[ê°€-í£]+[\d]{3,4}$', text): return True
    
    building_keywords = ['ìƒí—ˆê´€', 'ì‚°í•™', 'ê³µí•™ê´€', 'ì¸ë¬¸ê´€', 'ê³¼í•™ê´€', 'ì˜ˆìˆ ê´€']
    if any(bk in text for bk in building_keywords): return True

    return False

def get_center(bbox):
    (tl, tr, br, bl) = bbox
    return (tl[0] + tr[0]) / 2, (tl[1] + bl[1]) / 2

def simple_text_dump(ocr_results):
    """í¬ìŠ¤í„°/ê³µì§€ì‚¬í•­ìš© ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë³€í™˜ (ì¢Œìƒë‹¨ -> ìš°í•˜ë‹¨)"""
    # Yì¢Œí‘œ(ìƒë‹¨) ê¸°ì¤€ ì •ë ¬ í›„, Xì¢Œí‘œ(ì¢Œì¸¡) ê¸°ì¤€ ì •ë ¬ (ëŒ€ëµì ì¸ ë¬¸ë‹¨ ìˆœì„œ)
    # bbox[0][1] = top-left y, bbox[0][0] = top-left x
    sorted_data = sorted(ocr_results, key=lambda r: (r[0][0][1], r[0][0][0]))
    
    texts = []
    for (bbox, text, prob) in sorted_data:
        if prob < 0.3: continue
        texts.append(text)
        
    return "\n".join(texts)

def geometric_grid_analysis(ocr_results):
    items = []
    for (bbox, text, prob) in ocr_results:
        if prob < 0.3: continue
        if is_header_or_noise(text): continue
        cx, cy = get_center(bbox)
        items.append({'text': text, 'cx': cx, 'cy': cy})

    if not items: return ""

    x_coords = [i['cx'] for i in items]
    min_x, max_x = min(x_coords), max(x_coords)
    width = max_x - min_x
    col_width = width / 5
    days = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri']

    y_coords = [i['cy'] for i in items]
    min_y, max_y = min(y_coords), max(y_coords)
    total_height = max_y - min_y
    if total_height < 50: hour_height = 50 
    else: hour_height = total_height / 10 

    schedule_buckets = defaultdict(list)
    for item in items:
        col_idx = int((item['cx'] - min_x) / col_width)
        if col_idx < 0: col_idx = 0
        if col_idx > 4: col_idx = 4
        
        delta_y = item['cy'] - min_y
        estimated_hour_offset = int(delta_y / hour_height)
        hour = 9 + estimated_hour_offset
        
        clean_txt = item['text'].replace("]", "").replace("[", "").replace("'", "").strip()
        if clean_txt:
            schedule_buckets[(col_idx, hour)].append(clean_txt)

    mapped_data = []
    sorted_keys = sorted(schedule_buckets.keys(), key=lambda k: (k[0], k[1]))
    
    for (col_idx, hour) in sorted_keys:
        texts = schedule_buckets[(col_idx, hour)]
        merged_text = " ".join(texts)
        
        day_str = days[col_idx]
        time_str = f"{hour:02d}:00"
        
        mapped_data.append(f"[{day_str} | {time_str}] {merged_text}")

    return "\n".join(mapped_data)

def aggressive_json_repair(json_str: str) -> dict:
    """JSON ë¬¸ìì—´ ë³µêµ¬ ì‹œë„"""
    json_str = re.sub(r'}\s*(?={)', '}, ', json_str)
    json_str = re.sub(r',\s*]', ']', json_str)
    for i in range(10):
        try:
            return json.loads(json_str)
        except json.JSONDecodeError as e:
            if "Expecting ',' delimiter" in e.msg:
                json_str = json_str[:e.pos] + ',' + json_str[e.pos:]
            elif "Expecting value" in e.msg:
                if json_str.strip().endswith(','): 
                    json_str = json_str.strip()[:-1]
                else: 
                    break
            elif "Extra data" in e.msg:
                return json.loads(json_str[:e.pos])
            else: 
                break
    raise json.JSONDecodeError("JSON ë³µêµ¬ ì‹¤íŒ¨", json_str, 0)

def extract_json_from_text(text: str) -> dict:
    """í…ìŠ¤íŠ¸ì—ì„œ JSON ì¶”ì¶œ"""
    text = re.sub(r"```(?:json)?", "", text)
    text = re.sub(r"```", "", text)
    start = text.find('{')
    if start != -1:
        try:
            return aggressive_json_repair(text[start:])
        except json.JSONDecodeError:
            pass
    return {}

def create_model_inference(model_id: str):
    """ì§€ì •ëœ ëª¨ë¸ IDë¡œ LLM ì¸ìŠ¤í„´ìŠ¤ ìƒì„±"""
    params = {
        GenParams.MAX_NEW_TOKENS: 2000,
        GenParams.TEMPERATURE: 0,
    }
    return ModelInference(
        model_id=model_id,
        params=params,
        credentials=credentials,
        project_id=project_id
    )

def parse_llm_response(generated_response: str) -> AIChatParsed:
    """LLM ì‘ë‹µ í…ìŠ¤íŠ¸ë¥¼ íŒŒì‹±í•˜ì—¬ ê°ì²´ë¡œ ë³€í™˜"""
    extracted_json = extract_json_from_text(generated_response)
    parsed_data = extracted_json if extracted_json else {}
    parsed_data.setdefault("intent", "SCHEDULE_MUTATION")
    parsed_data.setdefault("type", "UNKNOWN")
    parsed_data.setdefault("actions", [])
    
    # ì‹œê°„ í˜•ì‹ ë³´ì • (24:00:00 -> 23:59:59)
    for action in parsed_data.get("actions", []):
        payload = action.get("payload", {})
        for time_field in ["start_at", "end_at"]:
            if time_field in payload and payload[time_field]:
                payload[time_field] = payload[time_field].replace("T24:00:00", "T23:59:59")
    
    return AIChatParsed(**parsed_data)

def process_schedule_mode(ocr_result) -> AIChatParsed:
    """ [ëª¨ë“œ 1] ì‹œê°„í‘œ ì²˜ë¦¬ ë¡œì§ (Grid Analysis + Text Model) - ê°•ì˜(Lecture) ìƒì„± """
    # 1. ê²©ì ë¶„ì„ (ì¢Œí‘œ ê¸°ë°˜)
    structured_text = geometric_grid_analysis(ocr_result)
    
    # 2. ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ (í•™ê¸° ê¸°ê°„)
    today = datetime.now()
    # í˜„ì¬ ì›” ê¸°ì¤€ìœ¼ë¡œ í•™ê¸° ì‹œì‘/ì¢…ë£Œì¼ ì¶”ì •
    if today.month >= 9:  # 2í•™ê¸° (9ì›”~12ì›”)
        semester_start = f"{today.year}-09-01"
        semester_end = f"{today.year}-12-20"
    elif today.month >= 3:  # 1í•™ê¸° (3ì›”~6ì›”)
        semester_start = f"{today.year}-03-02"
        semester_end = f"{today.year}-06-20"
    else:  # 1~2ì›” (ê²¨ìš¸ë°©í•™ or ë‹¤ìŒ í•™ê¸° ì¤€ë¹„)
        semester_start = f"{today.year}-03-02"
        semester_end = f"{today.year}-06-20"

    # ìš”ì¼ ë§¤í•‘: Mon=0, Tue=1, Wed=2, Thu=3, Fri=4
    days_mapping = "Mon=0, Tue=1, Wed=2, Thu=3, Fri=4"

    # 3. ì‹œê°„í‘œ ì „ìš© í”„ë¡¬í”„íŠ¸ (ê°•ì˜ ìƒì„±)
    instructions = """
    1. **Lecture Schedule**:
       - If text has Weekday + Time: Create a LECTURE.
       - Title: Class/Course Name.
       - **MERGING RULES**: If a class name is split across consecutive time slots (e.g. 18:00 'Software...' and 19:00 '...Thinking'), MERGE them into ONE lecture with extended duration.
       
    2. **Week Format**:
       - Convert day names to numbers: Mon=0, Tue=1, Wed=2, Thu=3, Fri=4
       - If same course appears on multiple days, include all days in the week array.
    """

    prompt_text = f"""You are an AI Schedule Assistant. Extract LECTURE schedules from the text below.

[Semester Period]
start_day: {semester_start}
end_day: {semester_end}

[Day Mapping]
{days_mapping}

[TEXT DATA]
{structured_text if structured_text else "No text found."}

[INSTRUCTIONS]
{instructions}

[OUTPUT FORMAT]
{{
  "intent": "LECTURE_MUTATION",
  "type": "LECTURE",
  "actions": [
    {{
      "op": "CREATE",
      "payload": {{
        "type": "LECTURE",
        "title": "ê°•ì˜ëª… (í•œê¸€ë¡œ)",
        "start_time": "HH:MM:SS",
        "end_time": "HH:MM:SS",
        "start_day": "{semester_start}",
        "end_day": "{semester_end}",
        "week": [0, 2]
      }}
    }}
  ]
}}
OUTPUT ONLY VALID JSON. DO NOT ADD EXPLANATIONS.
"""
    # 4. Text ëª¨ë¸ ì‚¬ìš© (70b-instruct ë“±)
    model = create_model_inference(WATSONX_MODEL_ID_TEXT)
    response = model.generate_text(prompt=prompt_text)
    return parse_llm_response(response)

def process_poster_mode(ocr_result) -> AIChatParsed:
    """ [ëª¨ë“œ 2] í¬ìŠ¤í„° ì²˜ë¦¬ ë¡œì§ (Text Dump + Text Model) """
    # 1. ë‹¨ìˆœ í…ìŠ¤íŠ¸ ë‚˜ì—´ (ë¬¸ë§¥ ë³´ì¡´)
    structured_text = simple_text_dump(ocr_result)
    
    # 2. ë‚ ì§œ ì»¨í…ìŠ¤íŠ¸ (ì ˆëŒ€ ë‚ ì§œ ì‚¬ìš© ìœ ë„)
    today = datetime.now()
    dates_prompt = f"Today: {today.strftime('%Y-%m-%d')}\nNOTE: Use dates found in text explicitly."

    # 3. í¬ìŠ¤í„° ì „ìš© í”„ë¡¬í”„íŠ¸ (í•µì‹¬ ì¼ì •ë§Œ ì¶”ì¶œ)
    instructions = """
    This is a POSTER for an event (Contest, Recruitment, Activity).
    
    RULES:
    1. **OUTPUT ONLY 1-2 MEANINGFUL EVENTS** from distinct dates in the poster:
       - ì ‘ìˆ˜/ëª¨ì§‘ ë§ˆê°ì¼ (Application Deadline) - MOST IMPORTANT
       - í™œë™ ì‹œì‘ì¼ (Activity Start) - if clearly stated
       - ê²°ê³¼ ë°œí‘œì¼ (Result Announcement) - if clearly stated
       
    2. **DO NOT CREATE** preparation tasks like "ì§€ì›ì„œ ì‘ì„±". Only extract ACTUAL dates from the poster.
       
    3. **TITLE FORMAT (Korean)**:
       - Include event name + date type (e.g., "ì„œìš¸ì²­ë…„ì •ì±…ë„¤íŠ¸ì›Œí¬ ëª¨ì§‘ ë§ˆê°", "K-water ë´‰ì‚¬ë‹¨ í™œë™ ì‹œì‘")
       
    4. **SCORING**:
       - importance_score: 7 for deadlines, 5 for other dates
       - estimated_minute: 30-60
       
    5. **DATES**: Extract EXACT dates from text. Do not guess or use current year if year is specified.
    """

    prompt_text = f"""You are an AI Event Helper. Extract key schedule dates from the poster.
Output must be in KOREAN. Extract only REAL dates mentioned in the poster.

[Reference Dates]
{dates_prompt}

[TEXT DATA]
{structured_text if structured_text else "No text found."}

[INSTRUCTIONS]
{instructions}

[OUTPUT FORMAT]
{{
  "intent": "SCHEDULE_MUTATION",
  "type": "TASK",
  "actions": [
    {{
      "op": "CREATE",
      "payload": {{
        "title": "ì´ë²¤íŠ¸ëª… + ë§ˆê°/ì‹œì‘/ë°œí‘œ",
        "end_at": "ISO8601",
        "importance_score": 7,
        "estimated_minute": 40,
        "category": "ê³µëª¨ì „"|"ëŒ€ì™¸í™œë™"|"ê¸°íƒ€"
      }}
    }}
  ]
}}
OUTPUT ONLY VALID JSON. NO EXPLANATIONS.
"""
    # 4. Text ëª¨ë¸ ì‚¬ìš© (70b-instruct) - OCR í…ìŠ¤íŠ¸ ë¶„ì„
    model = create_model_inference(WATSONX_MODEL_ID_TEXT)
    response = model.generate_text(prompt=prompt_text)
    
    # 5. í¬ìŠ¤í„° ì „ìš© í›„ì²˜ë¦¬ (ë…¸ì´ì¦ˆ ê°•ë ¥ ì œê±°)
    parsed = parse_llm_response(response)
    exclude_keywords = ["ì‹¬ì‚¬", "ì„ ë°œ", "ë¬¸ì˜", "ë©´ì ‘", "ì„œë¥˜", "êµìœ¡", "OT", "ì‚¬ì „", "í˜œíƒ", "ì§€ì›ì„œ ì‘ì„±"]
    if parsed.actions:
        filtered_actions = [
            a for a in parsed.actions 
            if not any(k in a.payload.get('title', '') for k in exclude_keywords)
        ]
        parsed.actions = filtered_actions
        
    return parsed

@router.post("/analyze", response_model=APIResponse, response_model_exclude_none=True)
async def analyze_image_schedule(
    file: UploadFile = File(...),
    timezone: str = "Asia/Seoul"
):
    # OCR ëª¨ë“ˆ ì²´í¬
    if not OCR_AVAILABLE:
        return APIResponse(
            status=503, 
            message="ì´ë¯¸ì§€ ë¶„ì„ ê¸°ëŠ¥ì„ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. OCR ëª¨ë“ˆì´ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤."
        )
    
    try:
        image_bytes = await file.read()
        
        # 1. EasyOCR
        ocr_result = await run_in_threadpool(ocr_reader.readtext, image_bytes, detail=1)
        
        # 2. ì´ë¯¸ì§€ íƒ€ì… ê°ì§€
        image_type = detect_image_type(ocr_result)
        
        # 3. íƒ€ì…ë³„ ì²˜ë¦¬ ë¡œì§ ì™„ì „ ë¶„ë¦¬
        if image_type == 'schedule':
            ai_parsed_result = process_schedule_mode(ocr_result)
        else:
            ai_parsed_result = process_poster_mode(ocr_result)
            
        action_cnt = len(ai_parsed_result.actions)
        
        # ê°œì„ ëœ assistant_message ìƒì„±
        if action_cnt > 0:
            if image_type == 'poster':
                titles = [a.payload.get('title', '') for a in ai_parsed_result.actions]
                if action_cnt == 1:
                    assistant_msg = f"ğŸ“‹ í¬ìŠ¤í„° ë¶„ì„ ì™„ë£Œ! '{titles[0]}' ì¼ì •ì„ í™•ì¸í–ˆì–´ìš”."
                else:
                    assistant_msg = f"ğŸ“‹ í¬ìŠ¤í„° ë¶„ì„ ì™„ë£Œ! {action_cnt}ê±´ì˜ ì£¼ìš” ì¼ì •ì„ í™•ì¸í–ˆì–´ìš”."
            else:
                # ì‹œê°„í‘œ -> ê°•ì˜
                titles = [a.payload.get('title', '') for a in ai_parsed_result.actions[:3]]
                titles_text = ", ".join([t for t in titles if t])
                if titles_text:
                    assistant_msg = f"ğŸ“š ì‹œê°„í‘œ ë¶„ì„ ì™„ë£Œ! {action_cnt}ê°œì˜ ê°•ì˜ë¥¼ ë°œê²¬í–ˆì–´ìš”:\n{titles_text}{'...' if action_cnt > 3 else ''}\n\nì¶”ê°€í•˜ì‹œê² ì–´ìš”?"
                else:
                    assistant_msg = f"ğŸ“š ì‹œê°„í‘œ ë¶„ì„ ì™„ë£Œ! {action_cnt}ê°œì˜ ê°•ì˜ë¥¼ ë°œê²¬í–ˆì–´ìš”. ì¶”ê°€í•˜ì‹œê² ì–´ìš”?"
        else:
            assistant_msg = "ì¼ì •ì„ ì°¾ì§€ ëª»í–ˆì–´ìš”. ë‹¤ë¥¸ ì´ë¯¸ì§€ë¥¼ ì‹œë„í•´ì£¼ì„¸ìš”."

        return APIResponse(status=200, message="Success", data=ChatResponseData(
            parsed_result=ai_parsed_result,
            assistant_message=assistant_msg
        ))

    except Exception as e:
        print(f"Error: {str(e)}")
        return APIResponse(status=500, message=f"Server Error: {str(e)}")
